---
title: 'Class 3: Getting Used to Prediction'
output: pdf_document
date: "2025-09-02"
---

```{r, message=FALSE, warning=FALSE}
#--------------------------------------------------------
# POLSCI 797
#
# Intro to prediction
# Examples from marginaleffects
#--------------------------------------------------------
# you may need to run:
# install.packages("marginaleffects")
# install.packages("texreg")
library(marginaleffects)
library(tidyverse)
library(texreg)
dat <- read.csv("https://marginaleffects.com/data/impartiality.csv")
dat$X <- NULL

# Make a "nonsense" column
# Why? We should NOT expect this to have relationship
# with variables of interest. We'll check later.

dat$nchar <- as.numeric(nchar(dat$country))
quantile(dat$nchar)
```

# Warmup questions

1.  $X$ is an $n \times (p+1)$ matrix of covariates with an intercept
column. $\beta$ is a $(p + 1) \times 1$ vector, and $y$ is an $n
\times 1$ vector. What are the dimensions of $(X\beta)^Ty$?
- $1 \ntimes 1$, in other words a scalar.

2.  $A$ is a two (row) by three (column) matrix. The second row is
equal to 2 times the first row. Write this matrix $A$, and write
$A^T$. How would you describe the relationship between the columns
in $A^T$?

- $A^T$, the relationship between the columns is that they are linearly dependent. 

------------------------------------------------------------------------

# Predictions for Social Science

Rothstein and Teorell (2008) paper from `marginaleffects` vignette is described
as arguing: ``a critical component of good governance in a country is the
impartiality of its government institutions, or the degree to which state power
is exercised according to written laws rather than personal connections or
biases."  The variables they included:

- `impartial`: A binary indicator equal to 1 if a country's public officials are impartial in the performance of their duties.
- `equal`: A continuous scale from 0 to 100 where higher values indicate that resources like public goods and welfare policies are distributed more equally across society.
- `democracy`: A binary indicator equal to 1 when a country is a democracy.
- `continent`: The continent on which a country is located.

```{r}
# Fit example model: different from reading, same data
#--------------------------------------------------------
fit_lm <- lm(equal ~ impartial * democracy * nchar + continent,
  data = dat
)

summary(fit_lm)

texreg::screenreg(fit_lm)
```

**Question**: how would you interpret the coefficient on `impartial`?

Key: it depends on the values of the other variables! These can be very difficult to manage when you're handling multiple other values manually. We'll discuss how `marginaleffects` is a good alternative for this soon.

**Question**: How do we recreate OLS coefficients given proof?

Hints:

- Function `solve()` finds the inverse of a matrix.
- Function `t()` finds the transpose of a matrix.
- Matrix multiplication is done with `%*%`, not just `*`.

```{r}
# 0. Predictions on ORIGINAL data

# How to write (X^T)^-1 X^T Y

model.matrix(fit_lm)

# model.matrix is equal to making the X matrix of predictors without the outcome variable

X <- model.matrix(fit_lm)

y <- fit_lm$model[, "equal"] # to avoid having NA rows

# X^T X

t(X) * X # This is not matrix multiplication!!!!

X_TX <- t(X) %*% X
solve(X_TX)

# X_T y

round((solve(t(X) %*% X) %*% t(X) %*% y), 2)
```

# What good is a prediction?

You can use the `predict()` function to get predictions for given values of X:

```{r}
beta_hat <- (solve(t(X) %*% X) %*% t(X) %*% y)

predict(fit_lm, newdata = dat[1:3, ])
```

But, `marginaleffects` package offers a lot more too!

```{r}
predictions(fit_lm)
```

# Example: finding the "average" prediction for our data

When writing, you often want to present predicted values for "example" cases in your data.

For example: "the model predicts that, on average, democratic countries have an outcome of XX." 

```{r}
avg_predictions(fit_lm, by = "democracy") # mean of the predictions
avg_predictions(fit_lm, variables = "democracy") # the average case
```

Predictions are an extremely powerful tool, but you should be careful to understand exactly what you are predicting.

**Often, the most important thing is to always remember what values the other predictors are being held at!**

**Question**: what does an "average" case mean? **Hint**: you can interpret this in at least two ways:

1. Take prediction of an "average" unit: e.g. average over all covariates, then predict.
2. Take prediction for all units, then average.

Are these in general the same? Show whether or not they are using this data as an example.

```{r}
# 1. Predicting an average case, a representative country that doesn't really exist

avg_predictions(fit_lm, variables = "impartial")

mean(dat$impartial)

table(dat$democracy) # Democracy is the modal case

# Take the data and build an hypothetical avg country
predictions(fit_lm, newdata = "mean")

avg_predictions(fit_lm, variables = "democracy")
```

You can plot these too:

```{r}
plot_predictions(fit_lm, by = c("continent", "democracy"))
```

And, these are `ggplot()` objects so you can easily customize visualization:

```{r}
plot_predictions(fit_lm, by = c("continent", "democracy")) +
  theme_classic()
```

A lot more sophisticated stuff you can do in the package: hypothesis tests, predicted changes in variables, etc.

You may want to *contrast* predictions between cases, such as democracies vs. autocracies. Can make comparisons directly:

```{r}
avg_comparisons(fit_lm, variables = "democracy")
```

# Predictions can also help you understand your own model better

**Question**: If Democracies have much higher values of `equal` on average, why is it not significant in the model?

```{r}

```

# Issues with OLS and Next Steps

Motivates two recurring problems with OLS and our approach to prediction so far:

1. **OLS is always trying to minimize predictive error, no matter
what**: This means that coefficients and variances can sometimes
get very very large, because this helps shrink error in some cases
regardless of how "realistic" those predictions are.  

2. **Overfitting**:
Similarly, OLS will fit the data to whatever columns we ask it to,
whether or not there is any genuine ``strong" relationship. This
overfitting is made worse by the fact that we have been training
(fitting) and testing (evaluating) our model on the same data. Soon,
that will change!

**Illustration**: OLS coefficients / variances can blow up:

```{r}
#------------------------------------------------------
# make 80 fake, random "noise" columns
# when p (the number of parameters in the model) >> n

n <- nrow(dat)
p <- 80

dat_extra <- as.data.frame( # Making fake data
  replicate(p,
    {
      type <- sample(c("num", "str", "logical"), 1)
      if (type == "num") {
        rnorm(n)
      } else if (type == "str") {
        sample(letters[1:5], n, TRUE)
      } else {
        sample(c(TRUE, FALSE), n, TRUE)
      }
    },
    simplify = FALSE
  )
)

names(dat_extra) <- paste0("var", 1:p)
#------------------------------------------------------
```

```{r}
dat_full <- bind_cols(dat, dat_extra)
dat_full$country <- NULL

fit_full <- lm(equal ~ ., data = dat_full)

texreg::screenreg(fit_full) # R2 of 1!
```

**Illustration**: overfitting!

```{r}
# These random columns end up having a really strong fit
# with the data

# get the MSE for the light model
summary(fit_lm)

# and the model with everything
summary(fit_full)


y <- dat$equal
y_hat_full <- predict(fit_full, newdata = dat_full)
y_hat <- predict(fit_lm, newdata = dat)

# The estimation between y and y_hat_full are exactly the same
# even if the data made was all junk
cbind(y, y_hat, y_hat_full)

# This is due to the fact that the training and test data are the same

# MSE full model
mean((y - y_hat_full)^2) # The fact these ase small is FAKE NEWS

# MSE light model
mean((y - y_hat)^2)
```
